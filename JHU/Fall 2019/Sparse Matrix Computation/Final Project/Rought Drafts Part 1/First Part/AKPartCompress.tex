\documentclass[10pt,twocolumn,letterpaper]{article}
\begin{document}
\title{Supervised Deep Sparse Coding Networks:Research }
\author{Summary written by Ak}
\maketitle

\noindent\textbf{Summary: }
%Sparse coding networks (SCN) are an effective method for computer vision, however are on efficient on a restricted number of data set. Typically, a data set is not compatible with an SCN when that dataset exhibits natural noise, such as variations in object orientation. Specifically, the linear representation of sparse coding is unable to capture the nonlinear variations of the data set. As such, an SCN networks poor performance on large data sets, where they're common variabilities and anomalies. However, Deep Neural Networks rely on large data sets, as they thrive on their ability to extract essential features of an object from an abundant diverse set of examples. As such the idea behind this paper is to develop a deep sparse coding networks that can thrive conditions that SCN networks do not.This paper presents a novel deep sparse coding network that relies on nonnegative sparse coding and multilevel optimization. The purpose being able to efficiently extend the conventional sparse coding to multilayer architectures in order to extend the networks learning capacity. Multilayer sparse coding networks are useful for this problem as combines the advantages of both deep-layer architectures and sparse coding networks Deep architectures are useful as they ameliorate any information degeneracy, meanwhile sparse coding networks contain a more intuitive interpretation of the learned features. As such this paper uses a bottleneck module, which uses two specialized sparse coding layers: one with a wide dictionary, and another with a slim dictionary in order to reduce the number of learnable parameters without loss of performance. Furthermore, both the dictionaries and regularization parameter of theÂ  SCN are optimized using  an end-to-end supervised learning algorithm based on multilevel optimization.

This paper describes a deep sparse coding network, a novel deep network that extends the network learning capacity of typical sparse coding networks. This work was developed by Sun, Nasarabadi and Tran who developed the network thru use of bottleneck modules.


\noindent\textbf{Related Works: }
This model was develop using based of prior work of four approaches: (i) Approaches [12,13,14] are wide networks en-cooperated in order to take advantage the high dimensional latent features.(ii) Bottleneck shaped neural networks [10,14] are employed to reduce the dimensions of the incoming signal in order to reduce an over-fitting caused by the network.(iii) Multi-layered networks are implemented with the purpose of training the reconstructive dictionaries of each layer in a greedily layer-wise fashion[7,18].(iv)Finally, Supervised Dictionary Learning methods were  relied on in order to apply a fixed point differentiation and bi-level optimization, a supervised learning scheme for shallow sparse coding(23,24) 

%While the deep sparse coding network this is a novel idea, it was developed by using approached documented in other papers.
%Wide networks [12,13,14] are proposed in order to take advantage of the high dimensional latent features,meanwhile a wide network contains a significantly larger number[10,11,15] of learn able parameters compared to its slim network counterpart. As such, this paper focuses on regularizing the wide SNC via dimensional reduction and clustering using non negative sparse coding-based approach, in order to optimize the number of learn able parameters.
%The purpose of a bottleneck shaped neural network[10,14] is to reduce the dimensions of the incoming signal this will in turn reduce an over-fitting caused by the network. The approach proposed in this paper, is to simultaneously learn high dimension discriminate representation and low dimensional clustered features into a single network architecture with end-to-end supervised learning.Alternative approaches employ a deep semi-non negative matrix factorization[17] that trains a hierarchical network with the reconstruction loss.
%Multi-layer are typically implemented with a purpose of training the reconstructive dictionaries of each layer in a greedily layer-wise fashion [7,18]. Alternatively, another approach is to unfold and approximate the sparse  coding process with deep neural networks [5,9]. The approach proposed in this paper differs from these methods, such that they extend the conventional non-negative sparse coding to a multi-layer architecture without unfolding, allows for higher compatibility of off the shelf sparse recover algorithms,and trains the network in an unsupervised, semi-supervised, or supervised method by manually balancing the discriminate and reconstructive loss. This method was inspired by stacked auto-encoders [19] and CNN-based model with auxiliary reconstruction loss [20,21].
%Previous Supervised Dictionary Learning methods relied on applying a fixed point differentiation and bi level optimization, a supervised dictionary learning scheme for the shallow sparse coding [23,24]. This paper generalizes the single-layer supervised dictionary learning to multilayer network based on multilevel optimization.
    
\noindent\textbf{Data Sets: Experiments }
The proposed network was tested using the CIFAR-10, CIFAR-100.The proposed network, SCN-4, reported a classification error of 5.81\% for the CIFAR-10 data-set, and 19.93\% for CIFAR-100 data-set.Comparative models such as the NOMP,  reported a classification error  18.60\% for  CIFAR-10 and 39.92\% for CIFAR-100. Additionally, the SCN-4 networks has comparable results to the ResNet and SwapOut for both CIFAR-10 and CIFAR-100 datasets while using less layers. The ResNet-29 models,which has 29 layers reported a classification error of 3.58\% respectively for CIFAR-10 an error of  17.31\% for CIFAR-100. 


\end{document}
