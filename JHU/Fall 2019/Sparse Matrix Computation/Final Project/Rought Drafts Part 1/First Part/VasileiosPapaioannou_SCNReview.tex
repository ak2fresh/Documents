\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission
 

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Supervised Deep Sparse Coding Networks}

\author{Reviewed by AK Meiyappan, Vasileios Papaioannou\\ October 20, 2019}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both


\maketitle
%\thispagestyle{empty}


%%%%%%%%% BODY TEXT
\noindent\textbf{Summary:} This paper describes a deep sparse coding network that extends network's learning capacity compared to typical sparse coding networks. This is achieved through the use of bottleneck modules.

\noindent\textbf{Related work:} This model was developed based on four approaches: (i) Approaches [12,13,14] are wide networks encooperated in order to take advantage the high dimensional latent features.(ii) Bottleneck shaped neural networks [10,14] are employed to reduce the dimensions of the incoming signal in order to reduce an over-fitting caused by the network.(iii) Multi-layered networks are implemented with the purpose of training the reconstructive dictionaries of each layer in a greedily layer-wise fashion[7,18].(iv)Finally, Supervised Dictionary Learning methods were relied on in order to apply a fixed point differentiation and bi-level optimization, a supervised learning scheme for shallow sparse coding(23,24) 

\noindent\textbf{Approach:} The intuition in this paper is to expand feature space to get a fine tuned view of the features and then compress this space to collect the most important ones or a weighted version of them. This is achieved with a bottleneck module consting of an expansion and contraction part respectively. Each of these parts incorporates a corresponding dictionary helping to represent the input in terms of fundamental atoms. As an output, a code is constructed by solving an Elastic Network problem. Apparently, this approach introduces a regularization effect since the most prevalent features remain whenever dimension reduction takes place.

For this approach to work well independently of dataset size, there are several layers of bottleneck modules each of which outputs an encoded version of its input. For encoding, there exist a wide and a slim dictionary for expansion and contraction respectively. These dictionaries are updated and regularized accordingly as every other parameter in a deep network.

\noindent\textit{Datasets, Experiments and Results:} The proposed network was tested using the CIFAR-10, CIFAR-100.The proposed network, SCN-4, reported a classification error of 5.81\% for the CIFAR-10 data-set, and 19.93\% for CIFAR-100 data-set.Comparative models such as the NOMP, reported a classification error  18.60\% for  CIFAR-10 and 39.92\% for CIFAR-100. Additionally, the SCN-4 networks has comparable results to the ResNet and SwapOut for both CIFAR-10 and CIFAR-100 datasets while using less layers. The ResNet-29 models,which has 29 layers reported a classification error of 3.58\% respectively for CIFAR-10 an error of  17.31\% for CIFAR-100.

\noindent\textbf{Strengths:} SCN combines successfully the benefits of sparse coding and deep networks. It achieves state - of - the - art performance with much less parameters and layers compared to baseline approaches. As a result, it is fast and memory efficient. SCN works well in supervised, semi - supervised and unsupervised settings. It allows a much broader choice among the available sparse recovery algorithms while keeping the same network architecture. Moreover, in this model, regularization is naturally incorporated through the dimension reduction without imperative need for additional anti - overfitting techniques. Finally, SCN is conceptually close to stacked autoencoders and CNN - based models. These facts, make the proposed model flexible, straightforward and easy to understand.

\noindent\textbf{Weaknesses:} In this method, there are several issues about the dictionaries used. What the width of the wide and slim dictionaries should be and how to regularize these dictionaries are the main ones. Specifically, dictionary regularization affects network convergence and no guidance is provide to remedy this problem. As an approach, it is based on concepts that are still not that well understood in theory such as Batch Normalization or the feedforward pass. Finally, the paper lucks some future directions.

\noindent\textbf{Reflections:} Overall, SCN is a strong proof of concept that sparse representations and deep netwroks can work well together. Nevertheless, there are still some issues open in terms of how to set up this model. Apart from dictionary setup, some study could also be done about which sparse recovery algorithm is the best. It would interesting too, to what extent the bottleneck module could be incorporated as an independent block to an arbitrary network architecture.

{\small
	\bibliographystyle{ieee}
	\bibliography{egbib}
}

\end{document}