{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining\n",
    "\n",
    "10/10/2019\n",
    "\n",
    "**Hanyao(Amy) Qiu** - hqiu11@jhu.edu <br/>\n",
    "**Office Hour** - F 10:30am ~ 11:30am $~$ Whitehead 212 (Whitehead Hall Second Level Common Area) <br/>\n",
    "**Section**$~~~~~~$ - Th 4:30pm ~ 5:20pm $~~~$ Shaffer 302\n",
    "<br>\n",
    "\n",
    "## Section 6 \n",
    "\n",
    "- **Bayesian Inference**: Bayes' Theorem, Probabilitistic Model\n",
    "- **Classification**: KNN, Iris Dataset\n",
    "-  Q & A\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is stated mathematically as the following equation:\n",
    "\n",
    "$\\displaystyle P(A \\lvert B) = \\frac{P(A)\\,P(B \\lvert A)}{P(B)}$ \n",
    "\n",
    "where A and B are events and $P(B)\\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "> Among some observed objects 1% belongs to a special type, e.g., quasars mixed with many stars. Using a classification method 99% of these special objects can be correctly selected. This method also selects 0.5% of the other types of objects erroneously.\n",
    "\n",
    "> What is the probability of having a special type if an object is selected by the method?\n",
    "\n",
    "> #### Solution:\n",
    "\n",
    "> A: Have a special type\n",
    "> B: Selected by the method\n",
    "\n",
    "> $P(A) = 0.01$ <br>\n",
    "> $P(B|A) = 0.99$ <br>\n",
    "> $P(B|\\bar A) = 0.005$ <br>\n",
    "\n",
    ">$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\bar A)P(\\bar A)}\n",
    "        = \\frac{0.99 \\cdot 0.01}{0.99 \\cdot 0.01+0.99 \\cdot 0.005} = \\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilitistic Model\n",
    "\n",
    "- From data $D$ we can **infer** the parameters $\\theta$ \n",
    "\n",
    "> \\begin{eqnarray}\n",
    "    p(\\theta \\lvert D) = \\frac{p(\\theta)\\,p(D \\lvert \\theta)}{p(D)}\n",
    "     = \\frac{p(\\theta)\\,p(D \\lvert \\theta)}{\\int p(\\theta)\\,p(D \\lvert \\theta) \\, d\\theta}\n",
    "     \\propto p(\\theta)\\,p(D \\lvert \\theta),\n",
    "\\end{eqnarray}\n",
    "\n",
    "> where $Z = \\int p(\\theta)\\,p(D \\lvert \\theta) \\, d\\theta$ is a normalizing constant\n",
    "\n",
    "- Posterior $\\propto$ Prior $\\times$ Likelihood\n",
    "\n",
    "> The **posterior** is proportional to the **prior** times the **likelihood function** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\\displaystyle p(\\theta \\lvert D,M) = \\frac{p(\\theta \\lvert M)\\ p(D \\lvert \\theta,M)}{p(D \\lvert M)}$ \n",
    "\n",
    "> or\n",
    "\n",
    ">$\\displaystyle p(\\theta) = \\frac{\\pi(\\theta)\\,{\\cal{}L}(\\theta)}{\\int \\pi(\\theta)\\,{\\cal{}L}(\\theta)\\ d\\theta }$\n",
    "\n",
    "\n",
    "- data in the likelihood funtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function\n",
    "- A set of (conditionally) ***independent*** measurements\n",
    "\n",
    ">$\\displaystyle D = \\Big\\{x_i\\Big\\}_{i=1}^N$\n",
    "\n",
    "- The likelihood function is a product \n",
    "\n",
    ">$\\displaystyle {\\cal L}_D(\\theta) = p(D \\lvert \\theta) = p(\\{x_i\\} \\lvert\\,\\theta) = \\prod_{i=1}^N f(x_{i};\\theta) = \\prod_{i=1}^N \\ell_{i}(\\theta)$\n",
    "\n",
    "- Maximum Likelihood Estimation (MLE) \n",
    "\n",
    "> E.g., Gaussians... cf. method of least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Each female of age 65 or over in the General Social Survey was asked\n",
    "whether or not they were generally happy. Let $Y_i = 1$ if respondent $i$ reported\n",
    "being generally happy, and let $Y_i = 0$ otherwise. Our data have $n = 129$ independent individuals. \n",
    "> $\\theta$ is the probability that people felt happy.\n",
    "\n",
    "> our beliefs about $\\theta = \\sum_{i=1}^{N} Y_i / N $; (total population mean)\n",
    "\n",
    "> the model that, conditional on $\\theta$, the $Y_i$’s are i.i.d. binary random variables\n",
    "with expectation $\\theta$.\n",
    "\n",
    "The last item says that the probability for any potential outcome $\\{y_1, ..., y_{129}\\}$,\n",
    "conditional on $\\theta$, is given by\n",
    "> the probability i th person felt happy is $P(y_i)=\\theta^{y_i}(1-\\theta)^{1-y_i}$  （Bernoulli distribution）\n",
    "\n",
    "> so $P(y_1, ..., y_{129} | \\theta) = \\theta^{\\sum_{i=1}^{129} y_i} (1-\\theta)^{129 - \\sum_{i=1}^{129} y_i}$\n",
    "\n",
    "> $ P(\\theta | y_1, ..., y_{129}) = \\frac{P(y_1, ..., y_{129} | \\theta) P(\\theta)}{P(y_1, ..., y_{129})} \\propto P(y_1, ..., y_{129} | \\theta) P(\\theta) $\n",
    "\n",
    "What remains to be specified is our prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: numerical intergration in 1D \n",
    "\n",
    "> Implement Bayes' rule to infer a constant based on $N$ (independent) measurements\n",
    "\n",
    ">0. Assume Gaussian likelihood with $\\sigma=1$ and improper prior\n",
    "\n",
    ">0. Use function `np.trapz(f,x)` for numerical integration\n",
    "\n",
    ">0. Start from the code below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Try different priors in the numerical inference example! \n",
    "\n",
    "> Does the result change? \n",
    "><br><br>\n",
    "> How about with more or fewer measurements?\n",
    "\n",
    "- Implement the example using different integration techniques\n",
    "\n",
    "> Sample from prior density function or likelihood function to derive the expectation value and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\\displaystyle p(\\mu|D) = \\frac{\\pi(\\mu) \\prod {\\ell}_i(\\mu)}{\\int \\pi(\\mu) \\prod {\\ell}_i(\\mu)\\,d\\mu}\\ $ \n",
    "\n",
    ">$\\displaystyle \\ell_{i}(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\ \\exp\\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma_i^2}\\right\\}$\n",
    "\n",
    "> $\\pi(\\mu)$ -> prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prior \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(5) # fake data points from normal distribution\n",
    "mu = np.linspace(-2,2,1000) # grid over the parameter\n",
    "\n",
    "prior0 = 1\n",
    "lk = np.exp(-0.5*np.sum(np.square(data-mu[:,newaxis]),axis=1))\n",
    "pdf = prior0*lk / np.trapz(lk,mu) # Bayes' rule with improper prior 1\n",
    "\n",
    "plot(mu,pdf,'r'); xlabel('mu'); ylabel('posterior');\n",
    "np.trapz(mu*pdf,mu) # expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with different measurments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5,10,50]:\n",
    "    data0 = np.random.randn(i) # fake data points from normal distribution\n",
    "    lk = np.exp(-0.5*np.sum(np.square(data0-mu[:,newaxis]),axis=1))\n",
    "    pdf_i = lk / np.trapz(lk,mu) # Bayes' rule with improper prior 1\n",
    "    plt.plot(mu,pdf_i,'-',label=i)\n",
    "    print(np.trapz(mu*pdf_i,mu))\n",
    "xlabel('mu'); \n",
    "ylabel('posterior');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another improper prior\n",
    "\n",
    "Uniform but cannot be negative, e.g., temperature in Kelvin\n",
    "> $\n",
    "\\pi(\\mu) = \\left\\{ \\begin{array}{ll}\n",
    "        0 & \\mbox{if $\\mu < 0$} \\\\\n",
    "        1 & \\mbox{if $\\mu \\geq 0$} \n",
    "\\end{array}\\right. \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.ones_like(lk)\n",
    "prior[mu < 0] = 0\n",
    "\n",
    "numerator = prior * lk\n",
    "pdf0 = numerator / np.trapz(numerator,mu)\n",
    "\n",
    "plot(mu,pdf,'r')\n",
    "plot(mu,pdf0,'g--') \n",
    "xlabel('mu'); ylabel('posterior'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as gauss\n",
    "\n",
    "plot(mu,pdf, 'r', alpha=0.9,label =\"prior 1\")\n",
    "plot(mu,pdf0,'g--', alpha=0.9,label =\"prior uniform\") \n",
    "\n",
    "for s in [5,0.2]:\n",
    "    numerator = lk * gauss.pdf(mu,scale=s)\n",
    "    pdfG = numerator / np.trapz(numerator,mu)\n",
    "    plot(mu,pdfG,'--',lw=2, alpha=0.9,label=s)\n",
    "    print(np.trapz(mu*pdfG,mu))\n",
    "    \n",
    "xlabel('mu'); ylabel('posterior');\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intergration techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The numpy and scipy libraries include the composite trapezoidal (numpy.trapz) and Simpson's (scipy.integrate.simps) rules.\n",
    "\n",
    "- Here's a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import simps,cumtrapz\n",
    "from numpy import trapz\n",
    "\n",
    "x = np.linspace(1,25,1000)\n",
    "y = x\n",
    "\n",
    "# Compute the area using the composite trapezoidal rule.\n",
    "area = trapz(y, x)\n",
    "print(\"area =\", area)\n",
    "\n",
    "# Compute the area using the composite Simpson's rule.\n",
    "area = simps(y, x)\n",
    "print(\"area =\", area)\n",
    "\n",
    "# Cumulatively integrate y(x) using the composite trapezoidal rule.\n",
    "area = cumtrapz(y, x)\n",
    "print(\"area =\", area[-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[trapz](https://docs.scipy.org/doc/numpy/reference/generated/numpy.trapz.html)<br>\n",
    "[simps](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simps.html)<br>\n",
    "[cumtrapz](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.cumtrapz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Statistical Learning\n",
    "\n",
    ">|           -     | Supervised     |         Unsupervised     |\n",
    " |:---------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n",
    "\n",
    "- #### Classification\n",
    "\n",
    "> In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n",
    "\n",
    "> **Example**:\n",
    "An example would be assigning a given email into \"spam\" or \"non-spam\" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).\n",
    "\n",
    "> **Algorithms**:\n",
    "- Naive Bayes\n",
    "- $k$-NN\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis\n",
    "- Logistic regression\n",
    "- Decisions trees\n",
    "- Random forests\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$ Nearest Neighbors\n",
    "\n",
    "- Assign label or value based $k$ nearest neighbors ($k$-NN) in the training set\n",
    "\n",
    "> For example, the most frequent \"vote\" possibly with weighting\n",
    "\n",
    "> Simple but powerful\n",
    "\n",
    "> <img src=https://upload.wikimedia.org/wikipedia/commons/e/e7/KnnClassification.svg width=200>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance \n",
    "\n",
    "> - Euclidean Distance: \n",
    "$$\\displaystyle d(x, y) = \\sqrt{\\sum_{i=1}^{d}(x_i - y_i)^2}, \\text{ for any } x, y \\in \\mathbb{R}^d$$\n",
    "\n",
    "> - Manhattan Distance:\n",
    "$$\\displaystyle d(x, y) = \\sum_{i=1}^{d}\\left|x_i - y_i\\right|, \\text{ for any } x, y \\in \\mathbb{R}^d$$\n",
    "\n",
    "> - Minkowski Distance: \n",
    "$$\\displaystyle d(x, y) = \\left(\\sum_{i=1}^{d}\\left|x_i - y_i\\right|^p\\right)^{\\frac{1}{p}}, \n",
    "\\text{ for any } x, y \\in \\mathbb{R}^d$$\n",
    "which is a generalization of Euclidean and Manhattan distance.\n",
    "\n",
    "> You can choose the best distance metric based on the properties of your data. If you are unsure, you can do the experiment with different distance metrics and different values of k together and see which gives you the most accurate model. \n",
    "\n",
    "> The default setting of **sklearn.neighbors.KNeighborsClassifier** is \"metric=’minkowski’, p=2\", which is just the Euclidean distance.\n",
    "\n",
    "#### Weight\n",
    "\n",
    "> A refinement of the k-NN classification algorithm is to weigh the contribution of each of the k neighbors according to their distance to the query point $x_q$, giving greater weight $\\omega_i$ to closer neighbors. The classifier is \n",
    "**distance weighted k-NN algorithm**:\n",
    "\n",
    "> $$F(x_q)=arg\\max_{v \\in V} \\sum\\limits_{i=1}^k \\omega_i \\delta(v,y_i),$$\n",
    "\n",
    "> where the weight is defined by $$\\omega_i = \\frac{1}{d(x_i,x_q)^2}.$$\n",
    "\n",
    "> The default setting of **sklearn.neighbors.KNeighborsClassifier** is \"weights=’uniform’\". In this default setting, all points in each neighborhood are weighted equally. You can change it to \"weights=’distance’\" for this distance weighted k-NN algorithm. Then closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "#### Choice of $k$\n",
    "\n",
    "> The number of nearest neighbors, i.e., the value of $k$ need to be predefined. \n",
    "\n",
    "> In $k$-NN , finding the value of $k$ is not trivial. A small value of $k$ means that noise will have a higher influence on the result and a large value make it computationally expensive.\n",
    "\n",
    "> If you are using $k$ and you have an even number of classes (e.g., 2) it is a good idea to choose value of $k$ with an odd number to avoid a **tie**. \n",
    "\n",
    "> $k$ is a **hyperparameter** in $k$-NN algorithm, so in general, we can choose it via model selection (**e.g., cross-validation**). \n",
    "\n",
    "#### Curse of Dimensionality\n",
    "\n",
    "> \"Everybody is lonely in high dimensions\".\n",
    "\n",
    "> As the number of dimensions increases the volume of the input space increases at an exponential rate.  \n",
    "\n",
    "> The accuracy of $k$-NN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, neighbors\n",
    "\n",
    "% pylab inline\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15 \n",
    "\n",
    "iris = datasets.load_iris() # load the iris data\n",
    "X = iris.data[:,:2]  # we only take the first two features of the data\n",
    "y = iris.target # labels of data\n",
    "\n",
    "h = 0.01 # step size in the mesh\n",
    "\n",
    "# Points in a mesh of [x_min, m_max] x [y_min, y_max]\n",
    "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    \n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)# train\n",
    "    Z = clf.predict(grid)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    if False:\n",
    "        plt.scatter(xx, yy, c=Z, cmap=cmap_light, edgecolor='none')\n",
    "    else:\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max()); plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise / Unhomework \n",
    "\n",
    "- Which two features work best to predict the classes of the iris dataset?\n",
    "- How much better/worse than using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn import neighbors\n",
    "\n",
    "for k in [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3],[0,1,2,3]]:\n",
    "    print('For features:')\n",
    "    print(k)\n",
    "    X = iris.data[:,k] # using only 2 features for each\n",
    "    y = iris.target\n",
    "  \n",
    "\n",
    "    start = dt.datetime.now()\n",
    "    clf = neighbors.KNeighborsClassifier(5)\n",
    "    y_pred = clf.fit(X,y).predict(X)\n",
    "\n",
    "    print (\"Elapsed time\", dt.datetime.now()-start)\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "          % (iris.target.size, (iris.target!=y_pred).sum()))\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still cheating! Our training data and testing data are the same!!\n",
    "***You can get a subset in iris data as training data and remaining as testing data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo it！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "- From data $D$ we can **infer** the parameters $\\theta$ of model $M$ \n",
    "\n",
    ">$\\displaystyle p(\\theta \\lvert D) = \\frac{p(\\theta)\\,p(D \\lvert \\theta)}{p(D)}$ \n",
    ">\n",
    "\n",
    "1. Identify which part of the above equation is associated with prior, posterior, likelihood function and normalization factor?\n",
    "2. What's their physical meaning to researcher in an experiment respectively?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " P($\\theta$): prior<br>\n",
    " P($\\theta \\lvert D$): posterior<br>\n",
    " P(D|$\\theta$) is the likelihood (also a conditional probability), which we derive from our data<br>\n",
    " P(D) is a normalization constant to make the probability distribution sum to 1<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "In a TV Game show, a contestant selects one of three doors; behind one of the doors there is a prize, and behind the other two there are no prizes. After the contestant selects a door, the game-show host opens one of the remaining doors, and reveals that there is no prize behind it. The host then asks the contestant whether they want to SWITCH their choice to the other unopened door, or STICK to their original choice. Is it probabilistically advantageous for the contestant to SWITCH doors, assuming that the host selects a door to open, from those available, with equal probability. The probabilty of SWITCH is 2/3 and STICK is 1/3.\n",
    "\n",
    "1. Derive this result using Bayes Theroem.\n",
    "2. Confirm your result using Monte Carlo simulation with 1000 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](http://www.math.mcgill.ca/~dstephens/323/Handouts/Math323-01-MontyHall.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "1. Implement k nearest neighbor and naive bayes classfier on the below fruit identification dataset fruit_label using the last four column (https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt) with your choice of k for k mean. Do not use sklearn package, implement the equation using Python.\n",
    "2. Visualize your result.\n",
    "3. Show the calculation for the first row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fruits = pd.read_table('fruit_data_with_colors.txt')\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a fictional dataset that describes the weather conditions for playing a game of golf. Given the weather conditions, each tuple classifies the conditions as “Yes” or “No” for plaing golf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " | INDEX |  OUTLOOK  |   TEMPERATURE   |  HUMILITY  |  WINDY  |  PLAY GOLF  |\n",
    " |:-----:|:---------:|:---------------:|:----------:|:-------:|:-----------:|\n",
    " |0| Overcast | Hot | High | False | Yes |  \n",
    " |1| Overcast | Cool | Normal | True  | Yes |\n",
    " |2| Overcast | Mild | High | True | Yes |\n",
    " |3| Overcast | Hot | Normal | False  | Yes |\n",
    " |4| Rainy | Mild | High | False  | Yes |\n",
    " |5| Rainy | Cool | Normal | False  | Yes |\n",
    " |6| Rainy | Cool | Normal | True  | No |\n",
    " |7| Rainy | Mild | Normal | False  | Yes |\n",
    " |8| Rainy | Mild | High | True  | No |\n",
    " |9| Sunny | Hot | High | False  | No |\n",
    " |10| Sunny | Hot | High | True  | No |\n",
    " |11| Sunny | Mild | High | False  | No |\n",
    " |12| Sunny | Cool | Normal | False  | Yes |\n",
    " |13| Sunny | Mild | Normal | True  | Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided into two parts, namely, features and target. Features are ‘Outlook’, ‘Temperature’, ‘Humidity’ and ‘Windy’. Target is 'Play Golf'.\n",
    "\n",
    "The fundamental Naive Bayes assumption is that each feature makes an:\n",
    "\n",
    "- independent\n",
    "\n",
    "- equal\n",
    "\n",
    "contribution to the outcome.\n",
    "\n",
    "With relation to our dataset, this concept can be understood as:\n",
    "\n",
    "We assume that no pair of features are dependent. For example, the temperature being ‘Hot’ has nothing to do with the humidity.\n",
    "\n",
    "Secondly, each feature is given the same weight(or importance). For example, knowing only temperature and humidity alone can’t predict the outcome accuratey. None of the attributes is irrelevant and assumed to be contributing equally to the outcome.\n",
    "\n",
    "Note: The assumptions made by Naive Bayes are not generally correct in real-world situations. In-fact, the independence assumption is never correct but often works well in practice.\n",
    "\n",
    "$$P(A|B) = \\displaystyle{\\frac{P(B|A)P(A)}{P(B)}}$$\n",
    "\n",
    "where A and B are events.\n",
    "\n",
    "- Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n",
    "\n",
    "- P(A) is the priori of A (the prior probability, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event B).\n",
    "\n",
    "- P(A|B) is a posteriori probability of B, i.e. probability of event after evidence is seen.\n",
    "\n",
    "With regards to the dataset, we can apply Bayes' theorem in following way:\n",
    "\n",
    "$$\\displaystyle{P(y|X) = \\frac{P(X|y)P(y)}{P(X)}}$$\n",
    "\n",
    "where, y is target variable and X is a dependent feature vector (of size n) where:\n",
    "\n",
    "$$X = \\left(x_1, x_2, x_3, ..., x_n\\right)$$\n",
    "\n",
    "e.g. For the second row of dataset, $X = \\left(Rainy, Hot, High, True\\right)$, $Y = No$\n",
    "\n",
    "Then we'll have:\n",
    "$$P(y|x_1, \\cdots, x_n) = \\frac{P(x_1|y)P(x_2|y)\\cdots P(x_n|y)P(y)}{P(x_1)P(x_2) \\cdots P(x_n)}$$\n",
    "\n",
    "Since the denominator remains constant for a given input, we have:\n",
    "$$P(y|x_1, \\cdots, x_n) \\propto P(y) \\prod_{k=1}^n P(x_i|y)$$\n",
    "\n",
    "We need to create a classifier model. We find the probability of given set of inputs for all possible values of the target variable y and pick up the output with maximum probablity. This can be expressed as:\n",
    "$$\\displaystyle{y = argmax_y P(y) \\prod_{k=1}^n P(x_i|y)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll calculate $P(x_i|y)$.\n",
    "\n",
    "e.g $x_i$ = OUTLOOK\n",
    "\n",
    "|  OUTLOOK  |   Yes   |  No  |  P(Yes)  |  P(No)  |        \n",
    "|:---------:|:-------:|:----:|:--------:|:-------:|   \n",
    "|Sunny|2|3|2/9|3/5|\n",
    "|Overcast|4|0|4/9|0/5|\n",
    "|Rainy|3|2|3/9|2/5|\n",
    "|**Total**|**9**|**5**|**100%**|**100%**|\n",
    "\n",
    "Similarly, we can calculate other features $x_i$ (temperature, humidity and wind).\n",
    "\n",
    "Then calculate $P(Y)$\n",
    "\n",
    "|  Y=Play Golf  |  Count |  P(Yes) or P(No)  |        \n",
    "|:-------------:|:------:|:-----------------:|  \n",
    "|Yes|9|9/14|\n",
    "|No|5|5/14|\n",
    "|**Total**|**14**|**100%**|\n",
    "\n",
    "\n",
    "Let's test it on a new set of features(today's features):\n",
    "\n",
    "$X_{today} = (Sunny, Hot, Normal, False)$\n",
    "\n",
    "So, probablity of playing golf is given by:\n",
    "$P(Yes|today) = \\frac{P(Sunny|Yes)P(Hot|Yes)P(Normal|Yes)P(False|Yes)P(Yes)}{P(Today)}$\n",
    "\n",
    "$P(No|today) = \\frac{P(Sunny|No)P(Hot|No)P(Normal|No)P(False|No)P(No)}{P(Today)}$\n",
    "\n",
    "As a result,\n",
    "\n",
    "$P(Yes|today)\\propto P(Sunny|Yes)P(Hot|Yes)P(Normal|Yes)P(False|Yes)P(Yes)$\n",
    "\n",
    "$P(No|today)\\propto P(Sunny|No)P(Hot|No)P(Normal|No)P(False|No)P(No)$\n",
    "\n",
    "Then,\n",
    "\n",
    "$\\displaystyle{P(Yes|today) \\propto \\frac{2}{9}\\cdot \\frac{2}{9} \\cdot \\frac{6}{9} \\cdot \\frac{6}{9} \\cdot \\frac{9}{14} \\approx 0.0141}$\n",
    "\n",
    "$\\displaystyle{P(No|today) \\propto \\frac{3}{5}\\cdot \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot \\frac{2}{5} \\cdot \\frac{5}{14} \\approx 0.0069}$\n",
    "\n",
    "Also, $P(Yes|today) + P(No|today) = 1$\n",
    "\n",
    "We have\n",
    "\n",
    "$\\displaystyle{P(Yes|today) = \\frac{0.0141}{0.0141+0.0069}=0.67}$\n",
    "\n",
    "$\\displaystyle{P(No|today) = \\frac{0.0069}{0.0141+0.0069}=0.33}$\n",
    "\n",
    "\n",
    "As a result, given the features(Sunny, Hot, Normal, False), we can predict the target 'Play Golf' to be 'Yes'\n",
    "\n",
    "\n",
    "Reference: https://gerardnico.com/data_mining/naive_bayes#example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
