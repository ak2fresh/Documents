{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining\n",
    "\n",
    "10/17/2019\n",
    "\n",
    "**Hanyao(Amy) Qiu** - hqiu11@jhu.edu <br/>\n",
    "**Office Hour** - F 10:30am ~ 11:30am $~$ Whitehead 212 (Whitehead Hall Second Level Common Area) <br/>\n",
    "**Section**$~~~~~~$ - Th 4:30pm ~ 5:20pm $~~~$ Shaffer 302\n",
    "<br>\n",
    "\n",
    "## Section 7 \n",
    "\n",
    "- **Naive Bayes Classifier** \n",
    "- **QDA & LDA**\n",
    "- **Cross Validation**\n",
    "-  Q & A\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes Classifier</font></h1>\n",
    "\n",
    "- In general, we can use Bayes' rule (and law of total probability) to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "\n",
    ">$\\displaystyle P(C_k \\lvert\\,\\boldsymbol{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\boldsymbol{x}}(C_k)}{Z} $ \n",
    "\n",
    "\n",
    "- Naively assuming the features are independent \n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) = \\prod_{\\alpha}^d p(x_{\\alpha} \\lvert C_k)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Learning\n",
    "\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "\n",
    ">$\\displaystyle p(x_{\\alpha} \\lvert C_k) = G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})$\n",
    "\n",
    "- We have to also pick some prior for the classes\n",
    "\n",
    "> Using uniform or based on frequency of points in the training set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blackboard Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes](https://www.youtube.com/watch?v=8yvBqhm92xA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gaussian Naive Bayes\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.loadtxt('Class-Train.csv', delimiter=',')\n",
    "Q = np.loadtxt('Class-Query.csv', delimiter=',')\n",
    "print(D.shape)\n",
    "print(Q.shape)\n",
    "X, C = D[:,0:2], D[:,2]\n",
    "print(Q[:,0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(C)\n",
    "print ('There are %d classes:' % len(classes), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate feature means and variances for each class\n",
    "param = dict()  # we save them in this dictionary\n",
    "for k in classes: ## 0,1,2\n",
    "    members = (C == k) # boolean array\n",
    "    num = members.sum()    # True:1, False:0\n",
    "    prior = num / float(C.size)\n",
    "    X = D[members,:2] # slice out members\n",
    "    #print(X)\n",
    "    mu = np.mean(X,axis=0)      # calc mean\n",
    "    X -= mu\n",
    "    var = (X*X).sum(axis=0) / (X[:,0].size-1)\n",
    "    param[k] = (num, prior, mu, var) # save results\n",
    "    #print(param)\n",
    "    print (k,mu,var) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * ones(Q[:,0].size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(Q[:,0].size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = Q[i,:] - mu\n",
    "        d2 = diff*diff / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2*pi*var))\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X, C = D[:,0:2], D[:,2]\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X, C).predict(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same results from both methods?\n",
    "np.alltrue(k_pred[:1000]==y_pred[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster by getting rid of the `for` loops, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "iris = datasets.load_iris() \n",
    "classes = np.unique(iris.target) \n",
    "print ('There are %d classes:' % len(classes), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization \n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.scatter(iris.data[:,0], iris.data[:,1], c = iris.target, marker = 'o', s = 35) # plot data with true labels\n",
    "plt.colorbar() # 0: blue, 1: green, 2: red "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameters](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "# calculate feature means and variances for each class\n",
    "param = dict()  # save them in this dictionary\n",
    "for k in classes:\n",
    "    members = (iris.target == k) # boolean array\n",
    "    num = members.sum()    # True:1, False:0\n",
    "    prior = num / float(iris.target.size)\n",
    "    X = iris.data[members,:] # slice out members\n",
    "    mu = X.mean(axis=0)      # calculate the mean\n",
    "    X -= mu # normalize\n",
    "    var = (X*X).sum(axis=0) / (X[:,0].size-1) # sample variance\n",
    "    param[k] = (num, prior, mu, var) # save results: number of data, prior, mean, variance\n",
    "    if True: print (k, mu, var)\n",
    "        \n",
    "\n",
    "# init predicted values\n",
    "k_pred = -1 * ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu \n",
    "        d2 = diff*diff / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2*pi*var)) # posterior = prior * likelihood\n",
    "        if p > pmax:\n",
    "            pmax = p \n",
    "            kmax = k\n",
    "    k_pred[i] = kmax # predicted labels: index of the maximum a posterior\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum())) # number of mislabeled points\n",
    "        \n",
    "       \n",
    "endtime = datetime.datetime.now()\n",
    "print (\"time running is\")\n",
    "print ((endtime - starttime))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the predictions with original labels\n",
    "plt.figure(figsize=(20, 5))\n",
    "subplot(1,2,1)\n",
    "plt.scatter(iris.data[:,0], iris.data[:,1], c = iris.target, marker = 'x', s = 35) # plot data with true labels\n",
    "plt.colorbar()\n",
    "subplot(1,2,2)\n",
    "plt.scatter(iris.data[:,0], iris.data[:,1], c = k_pred, marker = 'o', s = 35) # plot data with predicted labels\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sklearn's version \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB() # Gaussian Naive Bayes Classifier\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) # predicted labels\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum())) # number of mislabeled points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Could you make codes run faster by getting rid of the `for` loops, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features are automatically treated correctly relative to each other\n",
    "\n",
    "> For example, measuring similar things in different units? 1m vs 1mm\n",
    "><br><br>\n",
    "> The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "- Independence is a strong assumption and for no good reason\n",
    "\n",
    "> Actually... it helps with the computational cost!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use  Naive Bayes \n",
    "\n",
    "> Naive Bayes performs well when we have multiple classes and working with text classification.\n",
    "\n",
    "> When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.\n",
    "\n",
    "\n",
    "\n",
    "##### They have several advantages:\n",
    "\n",
    "> 1. They are extremely fast for both training and prediction\n",
    "\n",
    "> 2. They provide straightforward probabilistic prediction\n",
    "\n",
    "> 3. They are often very easily interpretable\n",
    "\n",
    "\n",
    "> These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. \n",
    "> If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. \n",
    "> If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "##### Naive Bayes classifiers tend to perform especially well in one of the following situations:\n",
    "\n",
    "> 1. When the naive assumptions actually match the data (very rare in practice)\n",
    "\n",
    "> 2. For very well-separated categories, when model complexity is less important\n",
    "\n",
    "> 3. For very high-dimensional data, when model complexity is less important\n",
    "\n",
    "> The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: ***once you have enough data, even a simple model can be very powerful.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>QDA & LDA</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1)^T\\,\\Sigma_1^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1) + \\ln\\,\\lvert\\Sigma_1\\lvert$ vs.\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2)^T\\,\\Sigma_2^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2) + \\ln\\,\\lvert\\Sigma_2\\lvert$\n",
    "\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called [**Quadratic Discriminant Analysis**](https://scikit-learn.org/stable/modules/lda_qda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "><br><br>\n",
    "> Good, if we don't have enough data, for example...\n",
    "><br><br>\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose between LDA and QDA?\n",
    "\n",
    "> The difference is really a **bias-variance trade-off**. \n",
    "\n",
    "> The QDA estimates a separate covariance matrix for each class, so as the number of predictors becomes high, we experience a computational expense. Conversely, if we assume a common covariance matrix, we only have to do the computation once. LDA is a less flexible classifier than QDA, thus has substantially lower variance. However, if the assumption of uniform variance is highly off, then LDA can suffer high bias.     \n",
    "\n",
    "#### Relation with Gaussian Naive Bayes\n",
    "If in the LDA/QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA and LDA\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "><br><br>\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyQDA(object):    \n",
    "    def fit(self,X,C):\n",
    "        self.param = dict()\n",
    "        for k in np.unique(C):\n",
    "            members = (C==k)\n",
    "            prior = members.sum() / float(C.size)\n",
    "            S = X[members,:] # subset of class \n",
    "            mu = S.mean(axis=0)    \n",
    "            Z = (S-mu).T # centered column vectors\n",
    "            cov = Z.dot(Z.T) / (Z[0,:].size-1)\n",
    "            self.param[k] = (mu,cov,prior)\n",
    "        return self\n",
    "            \n",
    "    def predict(self,Y):\n",
    "        Cpred = -1 * ones(Y[:,0].size)\n",
    "        for i in range(Cpred.size):\n",
    "            d2min, kbest = 1e99, None\n",
    "            for k in self.param:\n",
    "                mu, cov, prior = self.param[k]\n",
    "                diff = (Y[i,:]-mu).T\n",
    "                d2 = diff.T.dot(linalg.inv(cov)).dot(diff) / 2\n",
    "                d2 += np.log(linalg.det(cov)) / 2 - np.log(prior) \n",
    "                if d2<d2min: d2min,kbest = d2,k\n",
    "            Cpred[i] = kbest\n",
    "        return Cpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference implementation\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "D = np.loadtxt('Class-Train.csv', delimiter=',')\n",
    "Q = np.loadtxt('Class-Query.csv', delimiter=',')\n",
    "print(D.shape)\n",
    "print(Q.shape)\n",
    "X, C = D[:,0:2], D[:,2]\n",
    "\n",
    "Cpred = MyQDA().fit(X,C).predict(Q)\n",
    "Cskit =   QDA().fit(X,C).predict(Q)\n",
    "\n",
    "print ('Number of different estimates:', (Cpred!=Cskit).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLDA(object):\n",
    "    \"\"\" Simple implementation for illustration purposes\n",
    "    \"\"\"       \n",
    "    def fit(self,X,C):\n",
    "        self.param = dict()\n",
    "        for k in np.unique(C):\n",
    "            members = (C==k)\n",
    "            num = members.sum()\n",
    "            prior = members.sum() / float(C.size)\n",
    "            S = X[members,:] # subset of class \n",
    "            mu = S.mean(axis=0)    \n",
    "            Z = (S-mu).T # centered column vectors\n",
    "            cov = Z.dot(Z.T) / (Z[0,:].size-1)\n",
    "            self.param[k] = (mu,cov,prior,num)\n",
    "        return self\n",
    "            \n",
    "    def predict(self,Y):\n",
    "        Cpred = -1 * ones(Y[:,0].size)\n",
    "        cov = 0\n",
    "        total = 0\n",
    "        for k in self.param:\n",
    "            cov += self.param[k][1]*(self.param[k][3]-1)\n",
    "            total += self.param[k][3]-1\n",
    "        cov /= total\n",
    "        for i in range(Cpred.size):\n",
    "            d2min, kbest = 1e99, None\n",
    "            for k in self.param:\n",
    "                mu, _, prior,_ = self.param[k]\n",
    "                diff = (Y[i,:]-mu).T\n",
    "                d2 = diff.T.dot(linalg.inv(cov)).dot(diff) / 2\n",
    "                d2 -=  np.log(prior) \n",
    "                if d2<d2min: d2min,kbest = d2,k\n",
    "            Cpred[i] = kbest\n",
    "        return Cpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference implementation\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "D = np.loadtxt('Class-Train.csv', delimiter=',')\n",
    "Q = np.loadtxt('Class-Query.csv', delimiter=',')\n",
    "\n",
    "X, C = D[:,:2], D[:,2]\n",
    "\n",
    "print(D.shape)\n",
    "print(Q.shape)\n",
    "Cpred = MyLDA().fit(X,C).predict(Q)\n",
    "Cskit =   LDA().fit(X,C).predict(Q)\n",
    "\n",
    "print ('Number of different estimates:', (Cpred!=Cskit).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cross-Validation</font></h1>\n",
    "\n",
    "- How to ***evaluate the quality of estimator***?\n",
    "\n",
    "> $k$-NN method's parameter affects the results\n",
    "\n",
    "- We saw on the IRIS data that 1-NN was overfitting\n",
    "\n",
    "> We discussed excluding the point itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do?\n",
    "- Partitions of the Training set\n",
    "\n",
    "- Random complementary subsets \n",
    "\n",
    "> Train on a larger subset, test on a small\n",
    "> <br>\n",
    "> Multiple rounds to decrease variance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $k$-fold cross-validation \n",
    "\n",
    "> 1. Create $k$ partitions of equal sizes, e.g., $k=2$ yields two subsets\n",
    "> 2. Pick a single partition and train on the other $(k\\!-\\!1)$ \n",
    "> 3. Repeat for all $k$ partitions - requires $k$ trainings\n",
    "\n",
    "- Leave-One-Out is a special case with $k=n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cross-Validation\n",
    "\n",
    "- Evaluate QDA on the [training](files/Class-Train.csv) set using 2-fold cross-validation\n",
    "\n",
    "> 1. What is the fraction of correct estimates? \n",
    "> 2. What is the uncertainty of that fraction?\n",
    " \n",
    "> The **training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.loadtxt('Class-Train.csv', delimiter=',')\n",
    "Q = np.loadtxt('Class-Query.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate([2,3]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dc = D.copy()\n",
    "# randomize and split to D1 + D2\n",
    "np.random.seed(seed=42)\n",
    "np.random.shuffle(Dc)\n",
    "split = int(Dc[:,0].size/2)\n",
    "D1, D2 = Dc[:split,:], Dc[split:,:]\n",
    "# train on one estimate on the other\n",
    "for i,(T,Q) in enumerate([(D1,D2),(D2,D1)]):\n",
    "    X, C = T[:,0:2], T[:,2]\n",
    "    Cpred, Ctrue = MyQDA().fit(X,C).predict(Q[:,:2]), Q[:,2]\n",
    "    print (\"Case #%d - Number of mislabeled points out of a total %3d points : %2d\" \\\n",
    "        % (i, Q.shape[0],(Ctrue!=Cpred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-fold CV - quick hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dc = D.copy()\n",
    "\n",
    "# randomize and split to D1 + D2\n",
    "np.random.seed(seed=42)\n",
    "np.random.shuffle(Dc)\n",
    "split = int(Dc[:,0].size/3)\n",
    "split2 = 2*split\n",
    "D1, D2, D3 = Dc[:split,:], Dc[split:split2,:], Dc[split2:]\n",
    "\n",
    "# train on one, estimate on the other\n",
    "for T,Q in [ (np.vstack([D1,D2]),D3), (np.vstack([D2,D3]),D1), (np.vstack([D3,D1]),D2)]:\n",
    "    Cpred = QDA().fit(T[:,:2],T[:,2]).predict(Q[:,:2])\n",
    "    Ctrue = Q[:,2]\n",
    "    print ((Cpred!=Ctrue).sum(), T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Implement LDA and compare to sklearn\n",
    "\n",
    ">0. Write code without using sklearn \n",
    ">0. Apply to [training](Class-Train.csv) and [query](Class-Query.csv) sets \n",
    ">0. Compare your results to sklearn's \n",
    "\n",
    "- Perform 10-fold cross-validation of *MyQDA* on [this](Class-Train.csv) file\n",
    "\n",
    ">0. Write code without using sklearn \n",
    ">0. Calculate average and variance of good classifications \n",
    ">0. Compare to sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.loadtxt('Class-Train.csv', delimiter=',')\n",
    "X = D[:,:2]\n",
    "C = D[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=10, shuffle=False) \n",
    "\n",
    "for k, (train, test) in enumerate(k_fold.split(X)):\n",
    "    print(k,np.shape(train),np.shape(test))\n",
    "    Cpred = MyQDA().fit(X[train],C[train]).predict(X[test])\n",
    "    print (k, ':\\t', (C[test]==Cpred).sum() / float(test.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold CV using sklearn package\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = QDA()\n",
    "cross_val_score(clf, X,C, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise / Unhomework from section 6\n",
    "\n",
    "- Which two features work best to predict the classes of the iris dataset?\n",
    "- How much better/worse than using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn import neighbors\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=False) \n",
    "\n",
    "for k in [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3],[0,1,2,3]]:\n",
    "    print('For features:')\n",
    "    print(k) \n",
    "    X = iris.data[:,k] # using only 2 features for each\n",
    "    y = iris.target\n",
    "    clf = neighbors.KNeighborsClassifier(5)\n",
    "    score = []\n",
    "    for p, (train, test) in enumerate(k_fold.split(X)):\n",
    "        Cpred = clf.fit(X[train],y[train]).predict(X[test])\n",
    "        temp = (y[test]==Cpred).sum() / float(test.size)\n",
    "        score.append(temp)\n",
    "        \n",
    "        #print(p, ':\\t', temp )\n",
    "    #print(score)\n",
    "    \n",
    "    crossv_score = sum(score)/10\n",
    "    \n",
    "    print(\"Cross validation score is : %f\"\n",
    "          % (crossv_score))\n",
    "    print(\"sklearn cross validation score is: %f\"\n",
    "         % (mean(cross_val_score(clf, X,y, cv=10))))\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
